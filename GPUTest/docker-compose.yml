version: '3.8'
services:
  vllm_benchmark:
    image: uhub.service.ucloud.cn/ing_aigc/benchmark:vllm-0.10.1-wan2.1-sd3image
    container_name: vllm_benchmark
    ports:
      - "8001:8000"
    volumes:
      - /home/ubuntu/.cache/modelscope/hub/models/deepseek-ai:/root/.cache/huggingface
      - ./results-9355-p2p:/vllm-workspace/results
    environment:
      - NCCL_P2P_LEVEL=SYS
      - NCCL_P2P_DISABLE=0
      # vLLM 0.10.1 优化配置
      - VLLM_USE_V1=0
      - VLLM_FLASH_ATTN_VERSION=2
      - VLLM_ATTENTION_BACKEND=FLASH_ATTN
      # - VLLM_CUDA_GRAPH_CAPTURE=1
      - ENFORCE_EAGER_MODE=false
      # GPU和模型配置
      - CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
      - MODEL_NAME=Qwen3-235B-A22B-Instruct-2507
      - TENSOR_PARALLEL_SIZE=8
      - GPU_MEMORY_UTILIZATION=0.9
      - PIPELINE_PARALLEL_SIZE=1
      # 性能优化参数
      - MAX_MODEL_LEN=0
      - MAX_NUM_BATCHED_TOKENS=0
      # - BLOCK_SIZE=16
      # - SWAP_SPACE=4
      - ENABLE_CHUNKED_PREFILL=false
      # - MAX_NUM_SEQS_PER_ITERATION=256
      # 数据集配置
      - DATASET_NAME=ShareGPT_V3_unfiltered_cleaned_split.json
      - NUM_PROMPTS=100
      - NUM_PROMPTS_LIST=3000 3000
      - SHAREGPT_OUTPUT=1024
      - OFFLINE_MAX_NUM_SEQS=128 256
    command: bash /vllm-workspace/start.sh
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
